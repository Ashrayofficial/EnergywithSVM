# -*- coding: utf-8 -*-
"""EnergyWITHSVM edited.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18tL1lOsmGqC5enkSZkVMofDdWo93ZF2J
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import scipy.stats as stats
from scipy.stats import norm
from scipy.stats import chi2_contingency
from scipy.stats import chi2
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
warnings.filterwarnings("ignore")
from tqdm import tqdm
from scipy import optimize
import random

FIGURE_SIZE = (15, 12)
SNS_FIGURE_SIZE = (20, 15)
RANDOM_STATE = 42
TEST_SIZE = 0.2

df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv')
df

df.isnull().sum()

"""# EDA"""

df.info()

#Converting data column to datetime type
df['date']= pd.to_datetime(df['date'])

datatypes = df.dtypes

datatypes
df['hour']=df.date.dt.hour

load_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv', parse_dates=['date'])
feature_set = load_df.drop(['rv1', 'rv2','lights', 'Appliances'], axis = 1)
target_set = load_df['Appliances']
fig, ax = plt.subplots(figsize=SNS_FIGURE_SIZE)
mask_matrix = np.triu(feature_set.corr())
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(feature_set.corr(), annot = True, cmap= cmap, linewidths=3, linecolor='black', mask=mask_matrix)

df.groupby(['hour']).mean()

plt.plot(df.groupby(['hour']).mean().index.values,df.groupby(['hour']).mean().Appliances.values)
plt.plot(df.groupby(['hour']).mean().index.values,df.groupby(['hour']).mean().lights.values)
plt.xticks(np.arange(0, 24, step=1))

plt.plot(df.groupby(['hour']).mean().index.values,df.groupby(['hour']).mean().T_out.values)
plt.plot(df.groupby(['hour']).mean().index.values,df.groupby(['hour']).mean().lights.values)
plt.yticks(np.arange(0, 26, step=2))
plt.xticks(np.arange(0, 24, step=1))
plt.figure(figsize=(40,20))
plt.show()

corr_col1 = ['T1', 'T2', 'T3', 'T4', 'T5', 'T7', 'T8', 'T9']


corr_col2 = ['RH_1', 'RH_2', 'RH_3', 'RH_4', 'RH_7']
corr_col3 = ['T6', 'T_out']
corr_col4 = ['RH_8', 'RH_9']

# Column division for the affecting parameters


temp = ["T1","T2","T3","T4","T5","T6","T7","T8","T9"]

humid = ["RH_1","RH_2","RH_3","RH_4","RH_5","RH_6","RH_7","RH_8","RH_9"]

weather = ["T_out", "Tdewpoint","RH_out","Press_mm_hg",
                "Windspeed","Visibility"]
lights = ["lights"]

plot_param = feature_set[temp + humid + weather ]

plot_param.describe()

f, ax = plt.subplots(2,2,figsize=(15,15))
viz1 = sns.distplot(plot_param["Press_mm_hg"],bins=10, ax= ax[0][0])
viz2 = sns.distplot(plot_param["T_out"],bins=10, ax=ax[0][1])
viz3 = sns.distplot(plot_param["Windspeed"],bins=10, ax=ax[1][0])
viz4 = sns.distplot(plot_param["Visibility"],bins=10, ax=ax[1][1])

train_df=feature_set.drop(['date'], axis=1)

train_df.describe()

"""# Decision trees"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split, GridSearchCV

X_train, X_test, y_train, y_test= train_test_split(train_df, target_set, test_size=0.2)

p = {'max_depth':np.arange(1,31), 'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'splitter': ['best', 'random']}


des=DecisionTreeRegressor()

x=GridSearchCV(des, p, cv=5, scoring='neg_median_absolute_error')

x.fit(X_train, y_train)

results = pd.DataFrame(x.cv_results_)

results[results['rank_test_score']==1]

des=DecisionTreeRegressor(splitter='random', max_depth=28, criterion='friedman_mse')

des.fit(X_train, y_train)

y_pred=des.predict(X_test)

y_pred

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print('MAE: ', mean_absolute_error(y_pred, y_test))
print('MSE: ', mean_squared_error(y_pred, y_test))
print('R2 Score: ', r2_score(y_pred, y_test))

"""# Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

p={'max_features': ['auto', 'sqrt', 'log2']}

des=RandomForestRegressor()

x=GridSearchCV(des, p, cv=5, scoring='neg_median_absolute_error')

x.fit(X_train, y_train)

results = pd.DataFrame(x.cv_results_)

results[results['rank_test_score']==1]

des=RandomForestRegressor(max_features='sqrt')

des.fit(X_train, y_train)

y_pred=des.predict(X_test)

y_pred

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print('MAE: ', mean_absolute_error(y_pred, y_test))
print('RMSE: ', mean_squared_error(y_pred, y_test))
print('R2 Score: ', r2_score(y_pred, y_test))

"""# SVM"""

class SoftMarginSVM:

    """Class that performs Soft margin SVM:
    ---------------
    Parameters:
    X-features
    y-target variable
    alpha-lagrangian multiplier
    w-weights(parameters which are learnt)
    C-regularization term

    """

    def __init__(self,X,y,C):
        self.alpha = None
        self.w = None
        self.supportVectors = None
        self.X=X
        self.y=y
        self.C = C

    def normalize(self,X):

        '''function to scale the train data'''

        mean=np.mean(X)
        std=np.std(X)
        X_norm=(X-mean)/std
        X_norm=self.addX0(X_norm)

        return X_norm,mean,std

    def normalizeTestData(self,X_test,train_mean,train_std):

        '''function to scale the test data'''

        X_norm=(X_test-train_mean)/train_std
        X_norm=self.addX0(X_norm)

        return X_norm


    def addX0(self,X):

        ''' function to add bias to the dataset'''

        return np.column_stack([np.ones([X.shape[0],1]),X])




    def fit(self, X, y):

        '''function for fitting the model'''

        N = len(y)

        # Gram matrix of (X,y)
        Xy = X*y[:,np.newaxis]

        GramXy = np.matmul(Xy,Xy.T)

        def Ld0(G, alpha):

            '''function to calculate the dual form of SVM'''
            obj_fn = alpha.sum() - 0.5*alpha.dot(alpha.dot(G))

            return obj_fn


            # Derivative of Lagrangian Function
        def partialDerivationLd0(G, alpha):

            '''function to calculate derivative of lagrangian function'''

            par_der = np.ones_like(alpha) - alpha.dot(G)
            return par_der

        alpha = np.ones(N)

        A = np.vstack((-np.eye(N), np.eye(N)))
        b = np.concatenate((np.zeros(N), self.C * np.ones(N)))

        constraints = ({'type': 'eq', 'fun':lambda a: np.dot(a,y), 'jac': lambda a: y },
                    {'type': 'ineq', 'fun':lambda a: b - np.dot(A, a), 'jac': lambda a: -A})

        optRes = optimize.minimize(fun = lambda a: -Ld0(GramXy, a),
                                x0 = alpha,
                                method = 'SLSQP',
                                jac = lambda a: - partialDerivationLd0(GramXy, a),
                                constraints = constraints)

        self.alpha = optRes.x


        self.w =  np.sum(( self.alpha[:, np.newaxis] * Xy), axis = 0)

        epsilon = 1e-4
        self.supportVectors = X[ self.alpha > epsilon]
        self.supportLabels = y[self.alpha > epsilon]


        self.b1 = self.supportLabels[0] - np.matmul(self.supportVectors[0].T, self.w)

        b = []
        for i in tqdm(range(len(self.supportLabels))):
            b_i = self.supportLabels[i] - np.matmul(self.supportVectors[i].T, self.w)
            b.append( b_i )

        self.b2 = sum(b)/len(b)

        self.intercept = self.b2

    def predict(self, X):

        '''function to return the predicted classes'''

        return 2*(np.matmul(X, self.w) + self.intercept > 0) - 1



        return accuracy,precision,recall,f1score

    def runModel(self):

        '''function to run the model'''

        self.X_train,self.X_test,self.y_train,self.y_test=train_test_split(self.X,self.y,test_size=0.2,random_state=0)

        #normalizing the data
        self.X_train,self.mean,self.std=self.normalize(self.X_train)
        self.X_test=self.normalizeTestData(self.X_test,self.mean,self.std)

        self.fit(self.X_train,self.y_train)

        y_hat_train = self.predict(self.X_train)

        print('Evaluation results for Training Date:\n')
        print('MAE: ', mean_absolute_error(y_hat_train, self.y_train))
        print('MSE: ', mean_squared_error(y_hat_train, self.y_train))
        print('R2 Score: ', r2_score(y_hat_train, self.y_train))
        print('\n\n')

        y_hat_test = self.predict(self.X_test)

        print('Evaluation results for Testing Dat:\n')
        print('MAE: ', mean_absolute_error(y_hat_test, self.y_test))
        print('MSE: ', mean_squared_error(y_hat_test, self.y_test))
        print('R2 Score: ', r2_score(y_hat_test, self.y_test))

model = SoftMarginSVM(X_train.values[:1000,:], y_train.values[:1000], C=1.5)
model.runModel()

